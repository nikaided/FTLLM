- 「ファインチューニングLLMクラブ」という名称はかっこ悪いため、以後は「FTLLMクラブ」と略記することにする。

# 目次
0. プログラムの実行環境について
1. FTLLMクラブの趣旨と運営方針
2. FTLLMクラブの参加要件
3. FTLLMクラブ開催の経緯と趣旨
4. 主催者の野望：自然言語処理×強化学習(RLHF)


# プログラムの実行環境について
当レポジトリのプログラムは.ipynbファイルに記述されています。勉強会ではkaggleのカーネル上で実行する予定ですが、ローカルで実行する場合は以下の手順をふんでください。なおGPUがないとつらいことも多いです。

```shell
cd FTLLM_CLUB
python -m venv .env
source .env/Scripts/activate # 各自のデバイスで仮想環境起動
pip -r requirements.txt
```

# FTLLMクラブの趣旨と運営方針
- 参加者がクオリティの高いLLMアプリケーションを開発できるようにコミュニティを形成するのがFTLLMクラブの趣旨です。
- FTLLMクラブはオフライン集会です(つまり参加者は実際にひとつの場所に集まります)
- 2週間に1回の頻度での開催を目論んでいます。
- 参加費は500円です。
- 参加者は各自でPCを持参していただきます。
- 最初の10分の間に主催者が自然言語分野や強化学習分野についてのプレゼンをおこないます。(プレゼンテーションの資料はのちほど添付しておきます)
- 主催者のプレゼンが終わると参加者はひとりで各々の作業をおこなうことができます(AES方式のWi-fiでインターネットへの接続が可能です)
- 参加者は主催者にいつでも質問することができます。質問内容は技術に関するものでも、漠然とした相談のようなもの構いません。主催者はそれらの質問に対していつも真摯に回答することが求められます。

# FTLLMクラブの参加要件
参加要件は以下のとおりです。

- プログラマー

# FTLLMクラブ開催の経緯と趣旨
今年に入ってからchatGPTをはじめとするLLM(大規模言語モデル)を土台にしたアプリケーションサービスが次々とリリースされ、個人的に度肝を抜かれました。これまでも自然言語処理技術を用いたアプリケーションは数多くリリースされてきましたが、そのどれもが物足りない印象でした。たとえば言語翻訳タスクにおいては、google翻訳が長い間第一線でがんばっていましたが、それでも英語ができる人にとってはほとんど使い道はありませんでした。Alexa, siriなどの音声対話型AIもパッとしない印象が顕著で、Amazonやappleのような大企業が巨額の資金を投入しても、満足のいくようなクオリティには達しませんでした。それがDeepLの登場で潮流が変わり始め、chatGPTの登場で一気に変わった。LLMというのはこれまでのアプリケーションとは異なり、個別のタスクだけをこなすものではありません。翻訳や対話のみならず、テキストのセンチメント分類、テキストの要約など、様々なタスクを非常に高い精度でこなす、汎用システムなのです。

LLMと一言で言ってもたくさんのモデルがあります。オープンソースのものもあればAPIだけが公開されているものもあります。英語で事前学習されたものも日本語で学習されたものもあります。毎月新しいモデルが公開され、それらのモデルをファインチューニングしたアプリケーションがリリースされています。またこれらLLMの公開と並行して、hugging faceのようなLLMハブの役割を持ったプラットフォームが興り、LangChainのような周辺機能を扱うようなライブラリも活発に開発されています。ＳＮＳでは毎日のようにLLMに関するTIPSが流れてきますが、情報量が多すぎてまるでついていけません。「アプリケーション開発のために本当に有益な情報は何なのか」という観点で情報をうまく整理し、実際のコードを手になじませることが重要なのかなと思いました。そういったことはひとりでやるには退屈なので勉強会の形をとることにしました。

それがFTLLMクラブ開催の経緯です。



# 主宰者個人の野望
主催者個人の方向性についても簡単に記しておきます。

- 機械学習エンジニア
- オープンエンドな(特定の質疑応答に特化したものではなく、幅広く様々な会話をおこなうような)チャットボットに興味があって今年の4月から勉強を始めました。
- 勉強をしていく中で、同じような目線、環境のひとたちと情報を共有したいが、大阪でそういうコミュニティを見つけるのは困難だと感じていた。
- ファインチューニングの手法として、教師あり学習や、自己教師あり学習ではなく、報酬最大化をベースとした強化学習に興味がある
- 以下の2つの論文を自分なりにアレンジして実装することを夏の目標としている
    - [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593)
    - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)


# 今後の進路
### 1. アプリケーション開発

企業にはそれぞれのドメインで解決する課題があり、自社開発であれ委託であれ、LLMを用いてそれを解決していく流れは容易に予想できる。個人開発者でも小規模な案件ならそういった進路はありうる。実際に自分もひとつ関わった案件がある。

- chatGPTのAPIを用いて膨大な社内ドキュメントのQ&Aを行うアプリケーション

顧客は存在しない(つまり経済的には価値はない)けれど、自分だけは盛り上がれるようなアツいアプリケーションを作ってリリースすることもできる。

### 2. ライブラリ開発

huggingfaceのtransformersライブラリに代表されるような、アプリケーション開発者を対象にした言語モデルのコアの部分の実装を担当するライブラリの開発に従事する。

### 3. 周辺機能開発

langChainに代表されるような、アプリケーション開発時に重宝するツールを作成する。

[langchain](https://github.com/hwchase17/langchain)

LangChainが取り組んでいる領域は主に６つあって、
1. プロンプトエンジニアリング
    - インプットトークンをどのようにテンプレ化するかという問題
    - インプットトークンの最大長をどれだけ広げるかという問題
2. チェーン
3. 
4. 
5. 
6. 
このあたりで力尽きた。

## 適切な問いをなげる
今後、自分が開発したいものを開発していくうえで、わからないことが多すぎる。答えはわからなくても、適切な問いを発することがとても大事。問いがうまくできれば、あとはそれをひたすら掘っていける。

### 今後問いが発生するであろう領域(思いついたら追加していく)
- 自分が作ろうとしているアプリケーションにふさわしい言語モデルはどれなのか？
    - オープンソース系で足りるか?商用化されたLLMのAPIを利用したほうがよいか？
        - [いちgoogle社員の見解](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
            - 要約すると、
            - facebookのLLaMAが流出してから潮流が変わった。オープンソース系の台頭が著しい。
        - [LLM WorkSheet](https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4)
    - 日本語に特化したLLMで実用に耐えるものは存在するのか？

- ファインチューニングの訓練データセットはどうやって収集するべきか？
- ファインチューニングの訓練環境はどのクラウドを使えばいいのか？
    - そもそも個人がLLMをファインチューニングするのって現実的なの？
    - 下位レイヤーのパラメータは凍結したほうがいいの？
    - [LoRA(Low Rank Adaptation)](https://arxiv.org/pdf/2106.09685)
- 本番運用環境には何を使えばいいのか？

こういった問いをひとつずつ掘り下げて潰していく過程を参加者の皆さんと共有したいと考えている。